# 概率统计期末复习

## 概率统计这门课到底在讲什么，有什么意义，为什么要学这个

自然界中的现象分为两种，一种是确定性现象，一种是不确定性现象，

有一些不确定现象，虽然在试验之前不知道结果，但是在大量重复试验下，结果呈现某种规律性。

概率统计就是研究这种规律性的学科。

任何科研都无法离开概率统计。



### 第一章、概率论的基本概念

一些概念：

- 随机试验

- 样本空间、样本点

- 随机事件

- 基本事件

- 必然事件

- 不可能事件

- 事件的关系和运算：相等，和运算，积运算，差事件，互斥事件，逆事件，对立事件

- 频率、概率

- 等可能概型、古典概型

- 放回抽样、不放回抽样

- 超几何分布、

- 条件概率

- 乘法定理

  

  

### 什么是条件概率?

条件概率就是：事件A发生的条件下事件B发生的条件概率：

其实就是由乘法定律变形而来
$$
P(B|A)=\dfrac{P(AB)}{P(A)}
$$

### 什么是划分？

将样本空间S划分为$B_1，B_2，\dots,B_n$，B之间没有重合

==划分==的概念很简单，也很重要，因为全概率公式和贝叶斯公式都和划分有关

### 什么是全概率公式？

当然和划分有关
$$
P(A)=P(A|B_1)P(B_1)+P(A|B_2)P(B_2)+\dots+P(A|B_n)P(B_n)
$$

### 什么是贝叶斯公式？

用条件概率公式再代入全概率公式即可
$$
P(B_i|A)=\dfrac{P(A|B_i)P(B_i)}{B_i\sum_{j=1}^n P(A|B_j)P(B_j)},i=1,2,\dots,n
$$

### 如何判断事件是否独立？

- 用定义：

$$
P(AB)=P(A)P(B)
$$

- $P(B|A)=P(B)$

- 若A与B相互独立，则$A$与$\overline B$,等都相互独立

- 三个事件A，B，C必须满足
  $$
  \begin{cases}
  P(AB)=P(A)P(B)\\
  P(AC)=P(A)P(C)\\
  P(BC)=P(B)P(C)\\
  P(ABC)=P(A)P(B)P(C)
  \end{cases}
  $$
  才能说明独立

***

## 第二章：随机变量及其分布

### 什么是随机变量

对于每一个样本点e，有一个数与之对应，$X（e）$，称为随机变量

### 什么是离散型随机变量？

全部可能值是有限多个或可列无限多个

#### 什么叫可列无限多个？

就是随机变量可以和自然数一一对应。

==就是随机变量所有可能结果的集合是可数集==

### 什么是分布律？

随机变量每种取值对应的概率

可以有多种表示方法，可以形如$P\{X=x_k\}=p_k,k=1,2,\dots$

也可以用表格表示（当然这种表示方法只限于离散型）

我们说变量X服从某种分布的时候，其实就是对应着一种类型的分布律

### 三种重要的分布（一定要记住，而且要知道符号表示，不能题目里出现了不知道是哪种）

- 【0-1】分布：
  $$
  P\{X=k\}=p^k(1-p)^(1-k),k=0,1(1<p<1)
  $$
  
- 二项分布$$X\sim b（n,p)$$

$$
P\{X=k\}=C_n^kp^kq^{n-k},k=0,1,2,\dots,n.
$$



- 泊松分布$$X\sim\pi(\lambda)$$

$$
P\{X=k\}=\dfrac{\lambda^ke^{-\lambda}}{k!}
$$

- 均匀分布$$X\sim U（a，b）$$

### 什么是泊松定理？

其实就是n趋于无穷的时候，泊松分布近似于二项分布
$$
lim_{n\rightarrow \infty}C_n^k
$$





## 第三章、多维随机变量及其分布

二维随机变量

分布函数，联合分布函数

离散型随机变量

联合分布律

连续性二维随机变量

n维随机变量

n维随机变量的分布函数

边缘分布

边缘概率密度

正态分布

条件分布

条件概率密度

相互独立的随机变量

随机变量 的函数的分布

有限个相互独立的正态随机变量的相互组合仍然服从正态分布

### 第四章、随机变量的数字特征

数学期望

方差
$$
D(X)=Var(X)=E\{[x-E(X)]^2\}
$$
 方差是刻画X取值==分散程度==的一个量

方差的性质
$$
D(C)=0\\
D(CX)=C^2D(X)\\
D(X+C)=D(X)\\
D(X+Y)=D(X)+D(Y)+2E\{(X-E(X))(Y-E(Y))\}\\
若X，Y相互独立，则有\\
D(X+Y)=D(X)+D(Y)
$$


对于连续型随机变量，按（1.4）式有
$$
D(X)=\int_{-\infty}^{\infty}[x-E(X)]^2f(x)dx
$$
标准化变量
$$
X^*=\dfrac{X-\mu}{\sigma}
$$


### 各种分布的期望和方差

#### $$X\sim b(n,p)$$

$$
E(X)=np
D(X)=np(1-p)
$$

计算机方式是看成n次0-1分布来计算

#### 正态分布

先求标准正态变量
$$
E(Z)=0\\
D(Z)=1\\
E(X)=\mu\\
D(X)=\sigma^2
$$
正态分布完全由数学期望和方差决定







### 切比雪夫不等式

$$若X$具有数学期望E(X)=\mu,方差D(X)=\sigma^2，则对于任意正数\xi，不等式\\P\{|X-\mu|\geq \xi \}\leq\dfrac{\sigma^2}{\xi^2} $$,

### 切比雪夫不等式有什么意义

切比雪夫不等式给出了在随机变量的分布未知，而只知道期望和方差的情况下估计$$P\{|X-E(X)|<\xi\}$$的界限

这个估计是比较粗糙的，而且在已经知道分布的情况下可以准确算出来，这个不等式也就没什么用了。



### 协方差及相关系数

#### 协方差:

 称$$E\{[X-E(X)][Y-E(Y)]\}$$为随机变量X和Y的协方差，即为$$Cov(X,Y)$$

#### 相关系数

$$
\rho_{XY}=\dfrac{Cov(X,Y)}{\sqrt{D(X)}\sqrt{D(Y)}}
$$

#### 协方差的性质

$$
Cov(aX,bY)=abCov(X,Y),a,b是常数\\
Cov(X_1+X_2,Y)=Cov(X_1,Y)+Cov(X_2,Y)
$$





#### 相关系数的性质

- 小于1
- 等于一的充要条件是，Y是X的一次函数
- 当相关系数为0，称X和Y不相关
- 当X，Y相互独立，一定不相关，但是当X，Y不相关，却不一定相互独立，因为独立是就一般关系而言，而相关只是就线性关系而言

### 什么是矩、协方差矩阵

#### k阶原点矩

#### k+l阶混合矩

#### k+l阶混合中心矩

#### 协方差矩阵：n维随机变量的二阶混合中心矩

$$
C=\begin{bmatrix}
c_{11}&c_{12}&\dots c_{1n}\\
c_{21}&c_{22}&\dots c_{11}\\
\dots\\
c_{n1}&c_{n2}&\dots c_{nn}
\end{bmatrix}
$$



c12其实就是表示$$Cov(X_1,X_2)$$

一般来说，n维随机变量的分布是不知道的，或者是太复杂了锇，以至于在数学上不易处理，因此在实际应用中协方差矩阵就显得很重要。





## 第五章、大数定律及中心极限定理

### 大数定律

#### 弱大数定律

$$
lim_{n\rightarrow\infty}P\{|\dfrac{1}{n}\sum_{k=1}^{n}X_k-\mu|<\xi\}=1
$$

 证明会用到切比雪夫不等式

其实就是前n个变量的均值均值收敛于期望



#### 伯努利大数定律

$$
f_A是n次独立重复试验中事件A发生的次数，p是事件A在每次试验中发生的概率，则对于任意正数\xi>0有\\
lim_{n\rightarrow\infty}P\{|\dfrac{f_A}{n}-p|\geq\xi\}=1
$$

大数定律就是说明，当试验次数很大时，频率就是会接近概率，就可以用事件的频率代替事件的概率

### 中心极限定理

#### 定理一

有n个相互独立，服从同一分布的随机变量，已知数学期望$$\mu和方差\sigma^2$$

对于这些随机变量之和的标准化变量$$Y_n=\dfrac{\sum_{k=1}^{n}X_k-E(\sum_{k=1}^{n}X_k)}{\sqrt{D(\sum_{k=1}^nX_k)}}=\dfrac{\sum_{k=1}^nX_k-n\mu}{\sqrt{n}\sigma}$$

这个变量有一个分布函数$$F_n(x)$$

当n区域无穷的时候
$$
lim_{n\rightarrow\infty}F_n(x)=\int _{-\infty}^{x}\dfrac{1}{\sqrt{2\pi}e^{-t^2/2}dt}=\Phi(x)
$$
这个定理说明了当n充分大的时候，随机变量之和趋近于正态分布

#### 定理二：李亚普诺夫定理

设随机变量$$X_1,X_2,X_3,\dots,X_n$$相互独立，它们具有数学期望和方差$$E(X_k)=\mu_k,D(X_k)=\sigma_k^2$$

记$$B_n^2=\sum_{k=1}^{n}\sigma_k^2$$



#### 定理三

之所以叫中心极限定理，是因为它在统计中非常重要，又因为包含极限，所以叫中心极限定理





## 第六章 样本及抽样分布

从第六章开始是数理统计的内容，前五章是概率论的内容

直方图和箱线图

### 箱线图

#### 样本分位数

$$0.25分位数x_{0.25}称为第一四分位数,又记为Q_1$$

$$0.75分位数x_{0.75},称为第三四分位数，又记为Q_3$$

箱线图就是可以体现最大值，最小值，中位数，第一四分位数，第三四分位数

箱线图可以反应中心位置和散步程度

#### 疑似异常值

修正箱线图



### 随机样本

数量指标，总体，个体，容量，有限总体，无限总体

### 抽样分布

统计量

均值

方差

标准差

k阶矩

k阶中心矩

经验分布函数

格里汶科定理

### $$ \chi^2分布$$

其实就是标准正态分布随机变量的平方和

概率密度

$$\chi^2(1)其实就是\Gamma(\dfrac{1}{2},2)$$

###  $$\chi^2(n)里面的n是什么意思$$

n表示的是自由度

卡方分布其实是n个独立的==标准正态随机变量==的平方和的分布



#### 性质：

- 可加性

- 数学期望和方差：
  $$
  E
  $$

### $$\Gamma分布$$

$f(x; \alpha, \beta) = \frac{1}{\beta^{\alpha}\Gamma(\alpha)} x^{\alpha-1} e^{-\frac{x}{\beta}}$

其中，$\Gamma(\alpha) = \int_0^{\infty} t^{\alpha - 1} e^{-t} dt$

#### 上分位点

$$
P\{\chi^2>\chi^2_{\alpha}(n)\}=\int_{\chi_{\alpha}^2(n)}^{\infty}f(y)dy=\alpha
$$

称点$$\chi^2_{\alpha}(n)为\chi^2(n)分布上的上\alpha分位点$$

### t分布

$$设X\sim N(0,1),Y\sim\chi^2(n),且X，Y相互独立，称随机变量t=\dfrac{X}{\sqrt{Y/n}}$$服从自由度为n的t分布

t分布又称学生氏分布，概率密度为

当n充分大的时候，t分布近似于N（0，1）分布



### F分布

$$设U\sim \chi^2(n_1),V\sim \chi^2(n_2),且U，V相互独立，则称随机变量\\F=\dfrac{U/n_1}{V/n_2}$$

F分布也有上分位点

### 正态总体的样本均值于样本方差的分布

定理一 $$设X_1,X_2,X_3,X_n是来自正态总体N(\mu,\sigma^2)的样本，\overline X是样本均值，则有\\ \overline X\sim N(\mu,\sigma^2/n)$$

对于正态总体，$$N(\mu,\sigma^2)$$的样本均值$\overline X,S^2分别是样本均值和样本方差，有一下两个定理$

定理二 

- $\dfrac{(n-1)S^2}{\sigma^2}\sim\chi^2(n-1)$
- $\overline X与S^2$相互独立

定理三

$\dfrac{\overline X-\mu}{S/\sqrt{n}}\sim t(n-1)$

定理四 暂略



### 第九章、方差分布及线性回归



## 参数估计

### 点估计

- 估计量
- 估计值

### 什么是矩估计法

点估计就是知道一组样本数据，知道分布函数形式，去估计未知参数

估计的方法有很多种，本质是构造一个统计量，用样本数据求出一个未知参数的近似值

而矩估计法就是说构造的这个统计量就是前k阶矩，这个k表示的是未知参数的数量

### 什么是最大似然估计

就是定义一个和估计量有关的函数，求这个函数的极值，认为极值点就是参数的估计值

$\prod$是乘积符号，对应求和符号

最大似然估计函数

对数似然方程组往往没有有限函数形式的解，需要用数值方法求近似解，常用算法是牛顿-拉弗森算法，有时也用拟牛顿算法

### 什么是置信区间

给定置信水平，置信区间不是唯一的，但是应该尽可能让置信区间的长度小，

